1장
머신러닝의 종류
- 지도 학습: 레이블된 훈련 데이터에서 미래 데이터를 예측
	- 분류: 클래스 레이블 예측(스팸메일, 글씨 구분)
	- 회귀: 연솏적인 출력 값 예측(예측 변수와 결과 사이의 관계)
- 비지도 학습
	- 군집: 클러스터(서브그룹) 분류로 패턴을 찾음
	- 차원 축소
- 강화 학습: 환경에 따른 보상을 최대화 하도록 학습

특성: 데이터 테이블, 예측변수
타깃: 결과, 출력 등
손실함수: 비용함수

전처리: 데이터 형태 갖추기 -> 중복된 정보 제거 스케일 맞추기 등
예측 모델 훈련과 선택
	- 정확도: 정확히 분류된 샘플 비율
	-  훈련 데이터 셋 + 검증 데이터 셋 = 훈련 데이터
모델을 평가 및 본 적 없는 샘플로 예측

numpy, pandas, matploit, 사이킷런, 사이파이
https://github.com/gibutITbook/080311

2장
퍼셉트론 학습 규칙
1. 가중치를 0 또는 랜덤한 작은 값으로 초기화
2. 출력 값 계산 -> 가중치와 절편을 업데이트

퍼셉트론 파이썬 코드

붓꽃 데이터셋(https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)

아달린 파이썬 코드

3장
알고리즘을 훈련하기 위한 다섯 단계
1. 특성을 선택하고 훈련 샘플을 모은다.
2. 성능 지표를 선택한다.
3. 학습 알고리즘을 선택하고 모델을 훈련한다.
4. 모델의 성능을 평가한다
5. 알고리즘 설정을 바꾸고 모델을 튜닝한다

사이킷런 라이브러리
: 많은 학습 알고리즘 제공, 데이터 전처리, 세부 조정, 모델 평가 등의 기능 제공
문자열 형태의 클래스 레이블을 다룰 수 있다.
///
퍼셉트론 코드와 알고리즘 훈련 단계
///
클래스가 선형적으로 구분되지 않을 때 수렴할 수 없다

분류 오차 vs 정확도
1 - 오차 = 정확도

과대적합: 훈련 데이터에 있는 패턴은 감지하지만 본 적 없는 데이터에 일반화 되지 못한 것

로지스틱 회귀 : 분류 모델, 선형적으로 구분되는 클래스에 뛰어난 성능

오즈비(odds ratio): 특정 이벤트가 발생할 확률 P/(1-P) (P: 양성 샘플일 확률, 양성 샘플: 예측하려는 대상)

P:=p(y=1 | x) 특성이 x인 샘플이 클래스 1에 속할 조건부 확률

logit : odds ratio에 로그 함수를 취한 함수
logit(P) = log(P/1-P)
logit은 0과 1사이의 입력값을 받아 실수 범위 값으로 변환

로지스틱 시그모이드 함수: logit 함수를 거꾸로 뒤집은 함수

![[Pasted image 20250323035437.png]]

z: 가중치와 입력의 선형 조합으로 이루어진  최종 입력
![[Pasted image 20250323035559.png]]
0.5보다 크면 1, 아니면 0으로 해서 이진 출력으로 바꾸면 추정 가능
ex)비가 올지 안 올지 등 0.5 이상이면 온다 아니면 안온다

로지스틱 손실 함수의 가중치 학습
평균 제곱 오차 손실 함수
![[Pasted image 20250323040105.png]]
최대화하려는 가능도 L (각 샘플이 독립적이라 가정)
![[Pasted image 20250323040401.png]]
로그 가능도 함수
![[Pasted image 20250323040428.png]]
-> 예측이 잘못되면 손실이 무한대가 된다

완전 배치 경사 하강법

사이킷런을 사용하여 로지스틱 회귀 모델 훈련
상호 배타적인 클래스: 하나의 클래스에만 속할 수 있다
다중 레이블 분류: 훈련 샘플 하나가 여러 개의 클래스에 속할 수 있음
// 코드
사이킷런은 입력 데이터로 2차원 배열은 기대한다 -> numpy.reshape이용

규제를 사용하여 과대적합 피하기
과대적합: 훈련 데이터로는 잘 동작하지만, 한번도 본 적 없는 데이터로는 잘 일반화되지 않는 현상 -> 분산이 크다
과소적합 -> 편향이 크다
규제를 사용해 모델의 복잡도를 조정
규제: 과도한 파라미터(가중치) 값을 제한하기 위해 추가적인 정보를 주입하는 개념
L2 규제
![[Pasted image 20250323041744.png]]

람다가 증가하면 규제 강도가 높아짐-> 가중치 낮아짐
사이킷런의 LogisticRegression 클래스의 C 매개변수는 규제 하이퍼파라미터 람다의 역수 -> C를 감소 시키면 규제 강도가 높아짐 -> 가중치 낮아짐


서포트 벡터 머신을 사용한 최대 마진 분류
서포트 벡터 머신: 마진을 최대화하는 학습 알고리즘
마진: 클래스를 구분하는 초평면과 초평면에 가장 가까운 훈련 샘플 사이의 거리 -> 이러한 샘플이 서포트 벡터
![[Pasted image 20250323042358.png]]
최대 마진
큰 마진의 결정 경계는 일반화 오차가 낮아지는 경향이 있다
작은 마진의 모델은 과대적합되기 쉽다

슬랙 변수를 사용하여 비선형 분류 문제 다루기
C 값이 크면 오차에 대한 손실이 커짐
C 값이 작으면 분류 오차에 덜 엄격해짐
-> 마진 폭을 제어할 수 있다


![[Pasted image 20250323043021.png]]


커널 SVM을 사용하여 비선형 문제 풀기
선형적으로 구분되지 않는 데이터를 다루는 커널 방법: 매핑  함수를 사용하여 원본 특성의 비선형 조합을 선형적으로 구분되는 고차원 공간에 투영하는 것
![[Pasted image 20250323043312.png]]
-> 새로운 특성을 만드는 계산 손실이 매우 비싸다
-> 커널 기법
- 방사 기저 함수(가우스 커널)

결정 트리 학습
: 일련의 질문에 대한 결정을 통해 데이터를 분해하는 모델
루트에서 시작해 정보 이득이 최대가 되는 특성으로 데이터를 나눈다
노드가 많은 깊은 트리 -> 과대적합 -> 최대 깊이 제한(가지치기)

정보 이득 최대화
: 부모 노드의 불순도와 자식 노드의 불순도 합의 차이
자식 노드의 불순도가 낮을수록 정보 이득이 커진다
불순도 지표: 지니 불순도, 엔트로피, 분류 오차

결정 트리 만들기
결정 트리가 깊어질수록 결정 경계가 복잡해짐 -> 과대적합 -> 깊이 제한

랜덤 포레스트로 여러 개의 결정 트리 연결
랜덤 포레스트: 결정 트리의 앙상블, 여러 개의 결정 트리를 평균 내는 것
1. 훈련 데이터셋에서 중복을 허용하면서 랜덤하게 n개의 샘플을 선택
2. 부트스트랩 샘플(1번에서 선택한 샘플)에서 결정 트리를 학습
	1. 중복을 허용하지 않고 랜덤하게 d개의 특성을 선택
	2. 정보 이득과 같은 목적 함수를 기준으로 최선의 분할을 만드는 특성을 사용해서 노드를 분할
3. 단계 1~2를 k번 반복
4. 각 트리의 예측을 모아 다수결 투표로 클래스 레이블을 할당. 
-> 하이퍼파라미터 튜닝에 많은 노력을 기울이지 않아도 된다.
-> 가지치기도 필요없다
-> 부트스트랩 샘플의 크기가 작아지면 개별 트리의 다양성 증가 -> 무작위성 증가 -> 과대적합의 영향이 줄어듦 -> 성능이 줄어듦(테스트 성능이 감소)

k-최근접 이웃: 게으른 학습기
-> 인스턴스 기반 모델: 훈련 데이터셋을 메모리에 저장하는 것이 특징
1. 숫자 k와 거리 측정 기준을 선택
2. 분류하려는 샘플에서 k개의 최근접 이웃을 찾음
3. 다수결 투표를 통해 클래스 레이블을 할당
차원의 저주: 고정된 크기의 훈련 데이터셋이 차원이 늘어남에 따라 특성 공간이 점점 희소해지는 현상 -> 과대적합되기 쉬움

요약
로지스틱 회귀
서포트 벡터 머신
랜덤 포레스트
결정 트리
KNN

4장
좋은 훈련 데이터셋 만들기: 데이터 전처리

누락된 데이터 다루기
테이블 형태 데이터에서 누락된 값 식별
```python
dataFrame.isnull().sum() #null값이 있는지 true,false로 나타내고 각 컬럼의 총합을 구함
```
*pandas의 DataFrame을 사이킷런이 지원하긴 하지만  numpy 배열이 더 처리가 성숙하기 때문에 dataFrame.values를 통해 numpy 배열로 변환하여 사이킷런에 전달하는 것이 좋다*

```python
df.dropna(axis=0) #NaN이 있는 행(열: axis=1)을 
df.dropna(how='all') #모든 열이 NaN일 때만 삭제
df.dropna(thresh=4) #NaN이 아닌 값이 네 개보다 작으면 삭제
df.dropna(subset=['C']) # c열에 NaN가 있으면 삭제
```
제거를 할 때 너무 많은 데이터를 제거하면 안정된 분석이 안될 수도 있다
-> 누락된 값을 대체(보간)

누락된 값 대체
평균으로 대체
```python
imr = SimpleImputer(missing_values=np.nan, strategy='mean')
imr = imr.fit(df.values)
imputed_data = imr.transform(df.values)
```
strategy 매개변수로는 median(중간값), most_frequet(최빈값)이 있다

7페이지

```python
df.fillna(df.mean()) #위에 SimpleImputer과 같은 기능을 함
df.fillna(method='bfill') #누락된 값을 다음 행의 값으로 채움
df.fillna(method='ffill') #누락된 값을 이전 행의 값으로 채움
```

사이킷런 추정기 API
데이터에서 모델 파라미터를 학습
transform 메서드를 사용해 학습한 파라미터로 데이터 변환
SimpleImputer 클래스: 사이킷런의 변환기 API

범주형 데이터 다루기
순서가 있는 특성 매핑
- 범주형의 문자열을 정수로 변환하는 매핑 함수 작성

순서가 있는 특성 인코딩
- 수치적 크기에 대해 확신이 없거나 두 범주 사이의 순서를 정의할 수 없을 때 -> 임계 값을 사용하여 인코딩
- 데이터프레임의 apply 메서드로 인코딩하는 람다함수 적용
```python
df['x>M'] = df['size'].apply(lambda x: 1 if x in {'L', 'XL'} else 0)
df['x>L'] = df['size'].apply(lambda x: 1 if x == 'XL' else 0)
```

클래스 레이블 인코딩
순서가 없음 -> 아무 정수 할당
LabelEncoder 클래스 사용
fit_transform: fit 과 transform 메서드를 합쳐 놓은 단축 메서드

순서가 없는 특성: 원-핫 인코딩
분류기가 대체한 정수값에 의미를 부여할 수도 있다
원-핫 인코딩: 순서 없는 특성에 들어 있는 고유한 값마다 새로운 더미 특성을 만드는 것
원-핫 인코딩된 데이터셋을 사용할 때 다중 공선성 문제를 유념
-> 변수 간의 상관관계를 감소하려면 배열에서 한 특성 열을 삭제한다 -> 삭제하더라도 나머지 열로 유추 가능하므로 잃는 정보는 없다

```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OrdinalEncoder
ord_enc = OrdinalEncoder(dtype=int)
col_trans = ColumnTransformer([('ord_enc', ord_enc, ['color'])])
X_trans = col_trans.fit_transform(df)
X_trans
```
LabelEncoder의 경우 1차원 배열을 받아 한 개의 레이블을 인코딩하므로 여러 열을 같은 작업을 하려면 위와 같이 하면 된다.


데이터셋을 훈련 데이터셋과 테스트 데이터셋으로 나누기
model_selection 모듈의 train_test_split 함수를 사용하여 랜덤한 훈련 데이터셋과 테스트 데이터 셋으로 나누기
```python
from sklearn.model_selection import train_test_split
X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
X_trian, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)
#훈련 데이터셋과 테스트 데이터셋으로 분할
#test_size=0.3 -> 30%가 테스트 데이터셋에 할당
#stratify=y -> y의 비율을 동일하게 유지
```

특성 스케일 맞추기
대부분의 모델들에게 중요하다

StandardScaler: 표준화를 위한 클래스
RobustScaler: 이상치가 많이 포함된 작은 데이터셋에 유리 -> 과대적합되기 쉬운 알고리즘에 유리

유용한 특성 선택
과대적합, 일반화 오차를 감소시키는 방법
- 더 많은 훈련 데이터를 모은다
- 규제를 통해 복잡도를 제한한다
- 파라미터 개수가 적은 간단한 모델을 선택하낟
- 데이터 차원을 줄인다.

모델 복잡도 제한을 위한 L1규제와 L2규제
L1규제: 보통 희소한 특성 벡터를 생성->대부분의 특성 가중치가 0 -> 실제로 관련 없는 특성이 많은 고차원 데이터셋의 경우 이러한 희소성이 도움이 될 수 있음

```python
from sklearn.linear_model import LogisticRegression
LogisticRegression(penalty='l1', C=1.0, solver='liblinear', multi_class='ovr') #l1규제, C로 규제 강도 조절
```
희소성을 강하게 한다 -> C 매개변수 값을 낮춘다
-> 모든 가중치가 0이 된다

순차 특성 선택 알고리즘
차원축소: 규제가 없는 모델에서 특히 유용
- 특성 선택: 원본 특성에서 일부를 선택
- 특성 추출: 일련의 특성에서 얻은 정보로 새로운 특성을 만듦

순차 특성 선택(탐욕적 탐색 알고리즘)
초기의 차원을 특성 부분 공간으로 차원을 축소한다
문제에 가장 관련이 높은 특성 부분 집합을 자동으로 선택하는 것이 목적
관계없는 특성이나 잡음을 제거하여 계산 효율성을 높이고 모델의 일반화 오차를 줄임
- 순차 후진 선택: 모델 성능을 가능한 적게 희생하면서 초기 특성의 부분 공간으로 차원을 축소-> 과대적합에 유리
	- 알고리즘을 k=d로 초기화, d는 특성공간 X의 차원
	- 조건을 최대화하는 특성 x를 결정
	- 특성 집합에서 특성 x를 제거
	- k가 목표하는 특성 개수가 되면 종료, 아니면 2단게로

랜덤 포레스트의 특성 중요도 사용


#### 5장
##### 차원 축소를 사용한 데이터 압축

데이터셋의 정보를 요약하는 세 가지 기본적인 기술
- 주성분 분석을 사용한 비지도 데이터 압축
- 지도 방식의 차원 축소 기법인 선형 판별 분석을 이용하여 클래스 구별 능력 최대화하기
- 비선형 차원 축소와 시각화를 위한 t-SNE에 대한 간략한 소개

주성분 분석을 통한 비지도 차원 축소
특성 선택 -> 원본 특성을 유지
특성 추출 -> 새로운 특성 공간으로 데이터를 변환, 투영 -> 차원 축소 관점: 저장 공간 절약, 학습 알고리즘의 계산 효율성 향상, 차원의 저주 문제 감소(예측 성능 향상)

PCA(비지도 선형 변환 기법)
- 탐색적 데이터 분석(수집한 데이터가 들어왔을 때, 이를 다양한 각도에서 관찰하고 이해하는 과정), 주식 거래 시장의 잡음 제거, 생물정보학 분야에서 게놈 데이터나 유전자 발현 분석 등에 활용
- 특성 사이의 상관관계를 기반으로 데이터에 있는 패턴을 찾음

차원 축소를 위한 d(원본차원) x k(새 차원) 차원의 변환 행렬 W
특성 벡터 x
$$
x = [x_1, x_2, ..., x_d], x\in R^d
$$
변환 행렬을 곱해 차원 변환
$$
xW = z = [z_1, z_2, ..., z_k], z\in R^k
$$

- 원본 d 차원의 데이터를 새로운 k 차원의 부분공간으로 변환
- 모든 주성분이 다른 주성분들과 상관관계가 없으면(직교하면) 가장 큰 분산을 가짐

PCA는 스케일에 매우 민감 -> 특성 표준화 전처리 필요
PCA 알고리즘 단계
1. d 차원 데이터셋을 표준화 전처리
2. 공분산 행렬을 만듦
3. 공분산 행렬을 고유 벡터와 고윳값으로 분해
4. 고윳값을 내림차순으로 정렬하고 그에 해당하는 고유 벡터의 순위를 매김
5. 고윳값이 가장 큰 k개의 고유 벡터를 선택(k<=d)
6. 최상위 k개의 고유 벡터로 투영 행렬 W를 만듦
7. 투영 행렬 W를 사용해서 d차원 입력 데이터셋 X를 새로운 k차원의 특성 부분 공간으로 변환
(공분산 행렬: 대칭행렬) -> 고윳값: 실수, 고유 벡터는 서로 직교
표준화는 각 특성의 평균을 0으로 만들고 분산을 1로 조정하는 과정

공분산 행렬: dxd 차원의 대칭 행렬
- 특성 상호 간의 공분산을 저장
- 세 개의 특성으로 이루어진 공분산 행렬
$$
\Sigma = 
\begin{pmatrix}
\sigma_1^2 & \sigma_{12} & \sigma_{13} \\
\sigma_{21} & \sigma_{2}^2 & \sigma_{23}\\
\sigma_{31} & \sigma_{32}^2 & \sigma_{3}^2
\end{pmatrix}
$$
- 고유 벡터 = 주성분(최대 분산의 방향)
- 고윳값 = 주성분의 크기
(Wine 데이터셋: 13 x 13 차원의 공분산 행렬 -> 13개의 고유벡터, 고윳값)
$\Sigma \mathbf{v} = \lambda \mathbf{v}$
-> 람다: 고윳값, v: 고유벡터
numpy.linalg.eig() : 고윳값, 고유벡터 쌍 반환

총 분산과 설명된 분산
설명된 분산: 전체 고윳값의 합에서 해당 고윳값의 비율

특성 변환
데이터셋을 새로운 주성분 축으로 변환
1. 고윳값이 가장 큰 k개의 고유 벡터를 선택. (k: 새로운 특성 부분 공간의 차원)
2. 최상위 k개의 고유 벡터로 투영 행렬 W를 만듦
3. 투영 행렬W를 사용해서 d차원 입력 데이터셋 X를 새로운 k 차원의 특성 부분 공간으로 변환

계산 효율성과 모델 성능 사이의 절충점을 찾아 주성분 개수를 결정
선택한 고유벡터들로 투영 행렬 W를 생성

사이킷런의 주성분 분석

특성 기여도 평가하기
주성분에 대한 원본 특성의 기여도 = 로딩(loadings)
: 로딩은 고유 벡터에 고윳값의 제곱근을 곱한 값 

사이킷런의 PCA 객체에서 로딩값 얻기
```python
loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
```
pca.components_  = 고유벡터
pca.explained_variance_ = 고윳값


선형 판별 분석을 통한 지도 방식의 데이터 압축

선형 판별 분석(LDA): 규제가 없는 모델에서 차원의 저주로 인한 과대적합 정도를 줄이고 계산 효율성을 높이기 위한 특성 추출 기법
- 일반적인 개념은 PCA(비지도 선형 변환 기법)와 유사
- PCA는 데이터셋에 있는 분산이 최대인 직교 성분 축을 찾음. LDA는 클래스를 최적으로 구분할 수 있는 특성 부분 공간을 찾음

PCA vs LDA
PCA
- 비지도 학습

LDA
- 지도 학습
- 분류 작업에서 클래스 식별력이 더 뛰어남


선형 판별 분석(LDA)의 내부 동작 방식
1. d 차원의 데이터셋을 표준화 전처리(d: 특성 개수)
2. 각 클래스에 대해 d 차원의 평균 벡터를 계산
3. 클래스 간의 산포 행렬 $S_s$와 클래스 내 나포 행렬 $S_w$를 구성
4. $S_s, S_w$ 행렬의 고유 벡터와 고윳값을 계산
5. 고윳값을 내림차순으로 정렬하고 고유 벡터의 순서를 매김
6. 고윳값이 가장 큰 k개의 고유 벡터를 선택하여 d x k 차원의 변환 행렬 W를 구성
7. 변환 행렬 W를 사용하여 샘플의 새로운 특성 부분 공간으로 투영
-> PCA와 달리 LDA는 클래스 레이블(class label) 정보를 활용하여 최적의 특징 부분 공간을 찾는다
-> 데이터의 분산(variance)을 최대화하는 방향을 찾는 PCA와 달리, **클래스 간의 분리도(separability)**를 최대화하는 방향을 찾습니다

산포 행렬 계산
평균벡터 $\mathbf{m}_i$
만약 $n_i$개의 데이터가 클래스 $i$에 속하고 각 데이터 벡터가 $\mathbf{x}_j^{(i)}$라면
$\mathbf{m}_i = \frac{1}{n_i} \sum{\mathbf{x}_j \in \text{class } i} \mathbf{x}_j$

평균벡터를 사용하여 클래스 내 산포 행렬 $S_w$ 계산
$S_w = \displaystyle\sum_{i=1}^{c}{S_i}$

개별 산포 행렬 $S_i$를 산포 행렬 $S_w$로 모두 더하기 전에 스케일을 조정
-> 산포 행렬을 클래스 샘플 개수로 나누면 공분산 행렬을 계산 하는 것과 같음 -> 즉 공분산 행렬은 산포 행렬의 정규화 버전
$S_B = \sum_i = \displaystyle\sum_{i=1}^{c}{n_i(m_i - m)^T(m_i -m)}$

--
클래스 내 분산 행렬($S_W$): 각 클래스 내 데이터들의 분산을 나타냅니다. 각 클래스별 산포 행렬($S_i$)을 계산하여 합산합니다5 ...: $S_i = \sum_{\mathbf{x}_j \in \text{class } i} (\mathbf{x}_j - \mathbf{m}_i)(\mathbf{x}_j - \mathbf{m}i)^T$$S_W = \sum{i=1}^{c} S_i$ (여기서 $c$는 클래스 개수)

클래스 간 분산 행렬($S_B$): 각 클래스 평균 벡터들과 전체 평균 벡터 간의 분산을 나타냅니다5 ...: $\mathbf{m} = \frac{1}{N} \sum_{j=1}^{N} \mathbf{x}j$ (전체 평균 벡터, $N$은 전체 데이터 개수)$S_B = \sum{i=1}^{c} n_i (\mathbf{m}_i - \mathbf{m})(\mathbf{m}_i - \mathbf{m})^T$


새로운 특성 부분 공간을 위해 선형 판별 벡터 선택
PCA에서 공분산 행렬에 대한 고윳값 분해를 수행하는 대신 행렬 $S_w^{-1}S_B$ 의 고윳값을 계산
LDA에서 선형 판별 벡터는 최대 c-1개(c=클래스 레이블의 개수)

비선형 차원 축소와 시각화

- 고차원 데이터의 복잡성: 실제 데이터는 수많은 특징(feature)으로 구성되어 있어, 선형적인 방법으로는 데이터의 내재된 구조를 제대로 파악하기 어려울 수 있습니다.
- 비선형적 데이터 구조: 많은 실제 데이터는 클래스 경계나 데이터의 분포가 선형적이지 않은 복잡한 형태를 띱니다. 이러한 데이터에 선형 차원 축소 기법을 적용하면, 중요한 정보 손실이 발생하거나 데이터의 실제 구조를 왜곡할 수 있습니다.
- 데이터 시각화의 어려움: 고차원 데이터를 2차원 또는 3차원으로 시각화하여 직관적으로 이해하는 것은 어렵습니다. 비선형 차원 축소는 데이터의 복잡한 관계를 유지하면서 저차원으로 임베딩하여 시각화를 가능하게 합니다.

t-SNE를 사용한 데이터 시각화
t-SNE: 고차원 데이터의 점들 사이의 유사성을 저차원 공간에서의 확률 분포로 모델링하여, 유사한 데이터 포인트는 가까이, 다른 데이터 포인트는 멀리 배치하는 비선형 차원 축소 기법
- 고차원 공간에서 각 데이터 포인트에 대해 주변 데이터 포인트와의 유사성을 조건부 확률로 계산합니다. 가까운 이웃일수록 높은 확률 값을 가집니다.
- 저차원 공간에서도 각 데이터 포인트의 임베딩에 대해 유사성을 조건부 확률로 계산합니다.
- 고차원 공간과 저차원 공간에서의 이 두 확률 분포 사이의 차이를 최소화하는 방식으로 저차원 임베딩을 학습합니다. 이때 주로 **KL 발산(Kullback-Leibler divergence)**을 사용하여 차이를 측정합니다.

특징
- 비선형적인 데이터 구조를 잘 반영
- 지역적인 구조를 보존하는 데 강점 -> 고차원에서 가까운 점들은 저차원에서도 가깝게 유지되는 경향
- 주로 데이터 시각화에  사용


#### 6장
#####  모델 평가와 하이퍼파라미터 튜닝의 모법 사례

파이프라인을 사용한 효율적인 워크플로

파이프라인의 필요성: 머신러닝 워크플로우는 여러 단계의 전처리, 특징 추출, 모델 학습 단계를 포함합니다 . 각 단계를 개별적으로 처리하는 것은 번거롭고 오류가 발생하기 쉽습니다.
파이프라인(Pipeline)은 이러한 여러 단계를 하나의 통합된 프로세스로 묶어 관리함으로써 워크플로우를 간소화하고 효율성을 높입니다

- 파이프라인은 일련의 데이터 변환 단계와 최종 예측기(estimator)를 순차적으로 연결한 것
- 데이터는 파이프라인을 따라 순차적으로 처리되며, 각 단계의 출력이 다음 단계의 입력으로 전달
- sklearn 라이브러리는 파이프라인 구축을 위한 Pipeline 클래스를 제공

장점
- 코드 간결성 및 재사용성: 여러 단계를 하나의 파이프라인 객체로 묶어 코드를 간결하게 만들고, 구축된 파이프라인은 다른 데이터셋이나 실험에서도 재사용할 수 있습니다
- 실수 방지: 데이터 전처리 단계와 모델 학습 단계를 분리하지 않고 파이프라인 내에서 함께 처리함으로써, 훈련 데이터에 대해서만 전처리를 수행하고 테스트 데이터에는 훈련 데이터에서 학습된 변환을 적용하는 것을 보장하여 데이터 누출(data leakage) 위험을 줄입니다
- 편의성: 파이프라인 전체에 대해 fit 및 predict 메서드를 한 번만 호출하면 되므로, 각 단계를 개별적으로 호출하는 번거로움을 줄여줍니다
- 모델 튜닝 용이성: 파이프라인 내의 각 단계를 개별적인 하이퍼파라미터로 취급하여, 교차 검증과 함께 사용하여 최적의 파이프라인 구성을 효율적으로 탐색할 수 있습니다

k-겹 교차 검증을 사용한 모델 성능 평가
모델 성능 평가의 중요성
- 학습된 머신러닝 모델의 성능을 객관적으로 평가하는 것은 매우 중요합니다. 특히, 일반화 성능(generalization performance), 즉 학습에 사용되지 않은 새로운 데이터에 대한 예측 성능을 정확하게 추정해야 합니다.
- 단순히 훈련 데이터에 대한 성능만 평가하는 것은 과적합(overfitting) 문제를 간과할 수 있습니다

k-겹 교차 검증
- 주어진 데이터셋을 k개의 동일한 크기의 폴드(fold)로 나누고, (k-1)개의 폴드를 훈련 데이터로 사용하고 나머지 1개의 폴드를 검증 데이터로 사용하는 과정을 k번 반복하여 모델의 성능을 평가하는 기법
- 각 반복마다 다른 폴드가 검증 데이터로 사용되므로, 모든 데이터를 한 번씩 검증에 사용할 수 있습니다
- 최종 모델 성능은 k번의 검증 결과(예: 정확도, F1 점수 등)의 평균값으로 산출됩니다

장점
- 모든 데이터 활용: 데이터셋의 모든 부분을 훈련 및 검증에 활용하므로, 데이터가 부족한 경우에도 모델 성능을 보다 안정적으로 평가할 수 있습니다.
- 일반화 성능 추정의 신뢰성 향상: 여러 번의 검증을 통해 얻은 평균 성능은 단일 분할 검증보다 과적합에 덜 민감하며, 모델의 일반화 성능을 더 신뢰성 있게 추정할 수 있도록 돕습니다
- 모델 선택 및 하이퍼파라미터 튜닝에 활용: 다양한 모델이나 하이퍼파라미터 설정에 대해 k-겹 교차 검증을 수행하여, 가장 좋은 성능을 보이는 모델 및 파라미터 조합을 선택하는 데 활용할 수 있습니다.

sklearn에서의 k-겹 교차 검증
- sklearn.model_selection 모듈은 k-겹 교차 검증을 위한 다양한 클래스와 함수를 제공합니다 (KFold, StratifiedKFold, cross_val_score 등).
- cross_val_score 함수를 사용하면 모델, 데이터, 목표 변수, 폴드 수를 지정하여 간편하게 k-겹 교차 검증을 수행하고 각 폴드의 성능 점수를 얻을 수 있습니다